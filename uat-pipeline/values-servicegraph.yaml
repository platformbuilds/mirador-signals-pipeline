# Service Graph Collector Pipeline Configuration
# This collector receives traces from the gateway and generates service graph metrics
# Deploy with: helm install servicegraph ./uat-pipeline -f values-servicegraph.yaml

nameOverride: "servicegraph-collector"
fullnameOverride: ""

# Service Graph collector must be deployment with single replica
# Single replica ensures all traces are processed by same instance for complete service graph
mode: "deployment"

namespaceOverride: ""

# Backend destinations for processed telemetry
backends:
  # Traces backend (e.g., Tempo, Jaeger)
  traces:
    endpoint: "tempo.observability.svc.cluster.local:4317"
    tls:
      enabled: false
      insecure: true
      # ca_file: ""
      # cert_file: ""
      # key_file: ""
    auth:
      type: "none"  # Options: none, bearer, basic, apikey
      # bearer:
      #   token: ""
      # basic:
      #   username: ""
      #   password: ""
      # apikey:
      #   key: ""
      #   value: ""

  # Metrics backend for service graph metrics (e.g., Prometheus, Mimir)
  metrics:
    endpoint: "prometheus.observability.svc.cluster.local:4317"
    tls:
      enabled: false
      insecure: true
    auth:
      type: "none"

# Disable all presets - service graph collector doesn't need them
presets:
  logsCollection:
    enabled: false
  hostMetrics:
    enabled: false
  kubernetesAttributes:
    enabled: false
  kubeletMetrics:
    enabled: false
  kubernetesEvents:
    enabled: false
  clusterMetrics:
    enabled: false

configMap:
  create: true

# Service Graph Collector Configuration
config:
  # Service Graph Connector - generates service graph metrics from traces
  connectors:
    servicegraph:
      # Latency histogram buckets for service graph metrics
      latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 400ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
      # Additional dimensions to include in service graph metrics
      dimensions:
        - http.method
        - http.status_code
        - rpc.grpc.status_code
      # Store configuration for tracking spans
      store:
        ttl: 2s           # How long to wait for matching spans
        max_items: 10000  # Maximum number of edges to track
      cache_loop: 60m
      store_expiration_loop: 10s

  exporters:
    # OTLP exporter for traces - send to traces backend
    otlp/traces:
      endpoint: {{ .Values.backends.traces.endpoint }}
      tls:
        insecure: {{ .Values.backends.traces.tls.insecure }}
        {{- if .Values.backends.traces.tls.ca_file }}
        ca_file: {{ .Values.backends.traces.tls.ca_file }}
        {{- end }}
      {{- if eq .Values.backends.traces.auth.type "bearer" }}
      headers:
        authorization: "Bearer {{ .Values.backends.traces.auth.bearer.token }}"
      {{- else if eq .Values.backends.traces.auth.type "basic" }}
      headers:
        authorization: "Basic {{ printf "%s:%s" .Values.backends.traces.auth.basic.username .Values.backends.traces.auth.basic.password | b64enc }}"
      {{- else if eq .Values.backends.traces.auth.type "apikey" }}
      headers:
        {{ .Values.backends.traces.auth.apikey.key }}: {{ .Values.backends.traces.auth.apikey.value }}
      {{- end }}

    # OTLP exporter for service graph metrics
    otlp/metrics:
      endpoint: {{ .Values.backends.metrics.endpoint }}
      tls:
        insecure: {{ .Values.backends.metrics.tls.insecure }}
        {{- if .Values.backends.metrics.tls.ca_file }}
        ca_file: {{ .Values.backends.metrics.tls.ca_file }}
        {{- end }}
      {{- if eq .Values.backends.metrics.auth.type "bearer" }}
      headers:
        authorization: "Bearer {{ .Values.backends.metrics.auth.bearer.token }}"
      {{- else if eq .Values.backends.metrics.auth.type "basic" }}
      headers:
        authorization: "Basic {{ printf "%s:%s" .Values.backends.metrics.auth.basic.username .Values.backends.metrics.auth.basic.password | b64enc }}"
      {{- else if eq .Values.backends.metrics.auth.type "apikey" }}
      headers:
        {{ .Values.backends.metrics.auth.apikey.key }}: {{ .Values.backends.metrics.auth.apikey.value }}
      {{- end }}

  extensions:
    # The health_check extension is mandatory for this chart
    health_check:
      endpoint: ${env:MY_POD_IP}:13133

  processors:
    # Memory limiter to prevent OOM
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_percentage: 25

    # Batch processor for traces
    batch/traces:
      timeout: 10s
      send_batch_size: 1024

    # Batch processor for service graph metrics
    batch/metrics:
      timeout: 30s
      send_batch_size: 1000

  receivers:
    # OTLP receiver - receives traces from gateway
    otlp:
      protocols:
        grpc:
          endpoint: ${env:MY_POD_IP}:4317
        http:
          endpoint: ${env:MY_POD_IP}:4318

  service:
    telemetry:
      metrics:
        readers:
          - pull:
              exporter:
                prometheus:
                  host: ${env:MY_POD_IP}
                  port: 8888
    extensions:
      - health_check
    pipelines:
      # Traces pipeline - receives traces and fans out to servicegraph connector and backend
      traces:
        receivers:
          - otlp
        processors:
          - memory_limiter
          - batch/traces
        exporters:
          - servicegraph    # Generate service graph metrics
          - otlp/traces     # Send traces to backend

      # Metrics pipeline - receives service graph metrics from connector
      metrics/servicegraph:
        receivers:
          - servicegraph    # Connector acts as receiver for generated metrics
        processors:
          - batch/metrics
        exporters:
          - otlp/metrics

# Ports configuration
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
    appProtocol: grpc
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP

# Single replica for service graph - all traces must go to same instance
replicaCount: 1

# Resource limits for service graph collector
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

# Enable GOMEMLIMIT for better memory management
useGOMEMLIMIT: true

enableConfigChecksumAnnotation: true
podAnnotations: {}

podLabels: {}

# Service configuration
service:
  type: ClusterIP
  annotations: {}

# No autoscaling for service graph - must remain at 1 replica
autoscaling:
  enabled: false

# No HPA for service graph collector
rollout:
  strategy: RollingUpdate
  rollingUpdate: {}
