# Mirador Signals Pipeline - UAT Environment
# Single Helm release deploying multiple connected OTel collector pipelines

# Global configuration
global:
  image:
    repository: otel/opentelemetry-collector-contrib
    tag: ""  # Uses appVersion from Chart.yaml
    pullPolicy: IfNotPresent
  command:
    name: otelcol-contrib

# Pipeline configurations
# Each pipeline is deployed as a separate deployment within the same release
pipelines:
  # Gateway Pipeline - receives telemetry from applications
  gateway:
    enabled: true
    nameOverride: "gateway"
    mode: "deployment"
    replicaCount: 3

    # Resource limits for gateway
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

    # Gateway-specific configuration
    config:
      exporters:
        # Route traces to service graph collector
        otlp/servicegraph:
          endpoint: "mirador-servicegraph.observability.svc.cluster.local:4317"
          tls:
            insecure: true

        # Route logs to logs collector
        otlp/logs:
          endpoint: "logs-collector.observability.svc.cluster.local:4317"
          tls:
            insecure: true
          sending_queue:
            enabled: true
            num_consumers: 10

        # Route metrics to metrics collector
        otlp/metrics:
          endpoint: "metrics-collector.observability.svc.cluster.local:4317"
          tls:
            insecure: true

      extensions:
        health_check:
          endpoint: ${env:MY_POD_IP}:13133

      processors:
        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25

        batch/traces:
          timeout: 10s
          send_batch_size: 1024

        batch/logs:
          timeout: 1s
          send_batch_size: 10000

        batch/metrics:
          timeout: 60s
          send_batch_size: 8192

      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: ${env:MY_POD_IP}:4317
            http:
              endpoint: ${env:MY_POD_IP}:4318
        jaeger:
          protocols:
            grpc:
              endpoint: ${env:MY_POD_IP}:14250
            thrift_http:
              endpoint: ${env:MY_POD_IP}:14268
            thrift_compact:
              endpoint: ${env:MY_POD_IP}:6831
        zipkin:
          endpoint: ${env:MY_POD_IP}:9411
        prometheus:
          config:
            scrape_configs:
              - job_name: opentelemetry-collector
                scrape_interval: 10s
                static_configs:
                  - targets:
                      - ${env:MY_POD_IP}:8888

      service:
        telemetry:
          metrics:
            readers:
              - pull:
                  exporter:
                    prometheus:
                      host: ${env:MY_POD_IP}
                      port: 8888
        extensions:
          - health_check
        pipelines:
          traces:
            receivers: [otlp, jaeger, zipkin]
            processors: [memory_limiter, batch/traces]
            exporters: [otlp/servicegraph]

          logs:
            receivers: [otlp]
            processors: [memory_limiter, batch/logs]
            exporters: [otlp/logs]

          metrics:
            receivers: [otlp, prometheus]
            processors: [memory_limiter, batch/metrics]
            exporters: [otlp/metrics]

    # Port configuration for gateway
    ports:
      otlp:
        enabled: true
        containerPort: 4317
        servicePort: 4317
        protocol: TCP
        appProtocol: grpc
      otlp-http:
        enabled: true
        containerPort: 4318
        servicePort: 4318
        protocol: TCP
      jaeger-compact:
        enabled: true
        containerPort: 6831
        servicePort: 6831
        protocol: UDP
      jaeger-thrift:
        enabled: true
        containerPort: 14268
        servicePort: 14268
        protocol: TCP
      jaeger-grpc:
        enabled: true
        containerPort: 14250
        servicePort: 14250
        protocol: TCP
      zipkin:
        enabled: true
        containerPort: 9411
        servicePort: 9411
        protocol: TCP
      metrics:
        enabled: false
        containerPort: 8888
        servicePort: 8888
        protocol: TCP

    service:
      type: ClusterIP

    autoscaling:
      enabled: false

  # Service Graph Pipeline - generates service graph metrics from traces
  servicegraph:
    enabled: true
    nameOverride: "servicegraph"
    mode: "deployment"
    replicaCount: 1  # Must be 1 for complete service graph

    # Resource limits for service graph
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

    # Backend endpoints
    backends:
      traces:
        endpoint: "tempo.observability.svc.cluster.local:4317"
        tls:
          enabled: false
          insecure: true
        auth:
          type: "none"

      metrics:
        endpoint: "prometheus.observability.svc.cluster.local:4317"
        tls:
          enabled: false
          insecure: true
        auth:
          type: "none"

    # Service Graph specific configuration
    config:
      connectors:
        servicegraph:
          latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 400ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
          dimensions:
            - http.method
            - http.status_code
            - rpc.grpc.status_code
          store:
            ttl: 2s
            max_items: 10000
          cache_loop: 60m
          store_expiration_loop: 10s

      exporters:
        otlp/traces:
          endpoint: "tempo.observability.svc.cluster.local:4317"
          tls:
            insecure: true

        otlp/metrics:
          endpoint: "prometheus.observability.svc.cluster.local:4317"
          tls:
            insecure: true

      extensions:
        health_check:
          endpoint: ${env:MY_POD_IP}:13133

      processors:
        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25

        batch/traces:
          timeout: 10s
          send_batch_size: 1024

        batch/metrics:
          timeout: 30s
          send_batch_size: 1000

      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: ${env:MY_POD_IP}:4317
            http:
              endpoint: ${env:MY_POD_IP}:4318

      service:
        telemetry:
          metrics:
            readers:
              - pull:
                  exporter:
                    prometheus:
                      host: ${env:MY_POD_IP}
                      port: 8888
        extensions:
          - health_check
        pipelines:
          traces:
            receivers: [otlp]
            processors: [memory_limiter, batch/traces]
            exporters: [servicegraph, otlp/traces]

          metrics/servicegraph:
            receivers: [servicegraph]
            processors: [batch/metrics]
            exporters: [otlp/metrics]

    # Port configuration for service graph
    ports:
      otlp:
        enabled: true
        containerPort: 4317
        servicePort: 4317
        protocol: TCP
        appProtocol: grpc
      otlp-http:
        enabled: true
        containerPort: 4318
        servicePort: 4318
        protocol: TCP
      metrics:
        enabled: true
        containerPort: 8888
        servicePort: 8888
        protocol: TCP

    service:
      type: ClusterIP

    autoscaling:
      enabled: false

# Common settings
useGOMEMLIMIT: true
enableConfigChecksumAnnotation: true

# Service account configuration
serviceAccount:
  create: true
  automountServiceAccountToken: true

# No cluster role needed for gateway/servicegraph
clusterRole:
  create: false

# Disable all presets
presets:
  logsCollection:
    enabled: false
  hostMetrics:
    enabled: false
  kubernetesAttributes:
    enabled: false
  kubeletMetrics:
    enabled: false
  kubernetesEvents:
    enabled: false
  clusterMetrics:
    enabled: false
